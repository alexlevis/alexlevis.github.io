[{"categories":null,"content":"research areas and publications","date":"2024-08-03","objectID":"/research/","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Statistical Methodology \u0026 Theory ","date":"2024-08-03","objectID":"/research/:0:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Instrumental Variables Levis, A.W., Bonvini, M.*, Zeng*, Z., Keele, L., Kennedy, E.H. “Covariate-assisted bounds on causal effects with instrumental variables.” arXiv:2301.12106. Under revision for the Journal of the Royal Statistical Society: Series B. Levis, A.W., Kennedy, E.H., Keele, L. “Nonparametric identification and efficient estimation of causal effects with instrumental variables.” arXiv:2402.09332. In preparation for submission to Annual Review of Statistics and Its Application. Rakshit, P., Levis, A.W., Keele, L. “Local effects of continuous instruments without positivity.” arXiv:2409.07350. Submitted to the Journal of the Royal Statistical Society: Series B. Takatsu, K., Levis, A.W., Kennedy, E.H., Kelz, R., Keele, L, 2024. “Doubly-robust machine learning-based estimation methods for instrumental variables with an application to surgical care for cholecystitis.” Journal of the Royal Statistical Society: Series A, p. qnae089. doi:10.1093/jrsssa/qnae089. Zeng, Z., Levis, A.W., Lee, J., Kennedy, E.H., Keele, L. “Nonparametric estimation of local treatment effects with continuous instruments.” arXiv:2504.03063. ","date":"2024-08-03","objectID":"/research/:1:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Missing data Levis, A.W., Mukherjee, R., Wang, R., Fischer, H., Haneuse, S, 2024. “Double sampling for informatively missing data in electronic health record-based comparative effectiveness research.” Statistics in Medicine, 43(30), p. 6086-6098. doi:10.1002/sim.10298. Sun, S., Haneuse, S., Levis, A.W., Lee, C., et al, 2025+. “Estimating weighted quantile treatment effects with missing data by double-sampling.” arXiv:2310.09239. Biometrics (Accepted). Levis, A.W., Mukherjee, R., Wang, R., Haneuse, S, 2024. “Robust causal inference for point exposures with missing confounders.” Canadian Journal of Statistics, p. e11832. doi:10.1002/cjs.11832. Benz, L., Levis, A.W., Haneuse, S. “Comparing causal inference methods for point exposures with missing confounders: a simulation study.” arXiv:2407.06038. Submitted to BMC Medical Research Methodology. ","date":"2024-08-03","objectID":"/research/:2:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Transportability and Multi-Source Estimation Liu, Y., Levis, A.W., Normand, S. -L., Han, L, 2024. “Multi-source conformal inference under distribution shift.” arXiv:2405.09331. Proceedings of the 41-st International Conference on Machine Learning. PMLR 235:31344-31382. Liu, Y., Levis, A.W., Zhu, K., Yang, S., Gilbert, P.B., Han, L. “Targeted data fusion for causal survival analysis under distribution shift.” arXiv:2501.18798. Submitted to ICML 2025. Wang, G., Levis, A.W., Steingrimsson, J.A., Dahabreh, I.J. “Efficient estimation of subgroup treatment effects using multi-source data.” arXiv:2402.02684. Wang, G., Levis, A.W., Steingrimsson, J.A., Dahabreh, I.J. “Causal inference under transportability assumptions for conditional relative effect measures.” arXiv:2402.02702. ","date":"2024-08-03","objectID":"/research/:3:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Effect Estimation in Longitudinal Studies Loewinger, G.*, Levis, A.W.*, Pereira, F. “Nonparametric causal inference for optogenetics: sequential excursion effects for dynamic regimes.” arXiv:2405:18597. Under revision for the Journal of the American Statistical Association. Levis, A.W.*, Loewinger, G.*, Pereira, F, 2024. “Causal inference in the closed-loop: marginal structural models for sequential excursion effects.” arXiv:2405:18597v1. The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=BgZcuEsYU8. ","date":"2024-08-03","objectID":"/research/:4:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Optimal Policies Levis, A.W., Ben-Michael, E., Kennedy, E.H. “Intervention effects based on potential benefit.” arXiv:2405.08727. Submitted to the Annals of Statistics. ","date":"2024-08-03","objectID":"/research/:5:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Sensitivity Analysis Levis, A.W., Kennedy, E.H., McClean, A., Balakrishnan, S., Wasserman, L. “Stochastic interventions, sensitivity analysis, and optimal transport.” arXiv:2411.14285. In preparation for submission to the Journal of the American Statistical Association. Applied Research ","date":"2024-08-03","objectID":"/research/:6:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Electronic Health Records Koffman, L., Levis, A.W., Haneuse, S., Johnson, E., et al., 2021. “Evaluation of intensive telephonic nutritional and lifestyle counseling to enhance outcomes of bariatric surgery.” Obesity Surgery, 32, p. 133-141. doi:10.1007/s11695-021-05749-4. Koffman, L., Levis, A.W., Arterburn, D., Coleman, K.J., et al., 2021. “Investigating bias from missing data in an electronic health records-based study of weight loss after bariatric surgery.” Obesity Surgery, 31, p. 2125-2135. doi:10.1007/s11695-021-05226-y. ","date":"2024-08-03","objectID":"/research/:7:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Metascience Thombs, B.D., Levis, A.W., Azar, M., Saadat, N., et al., 2020. “Group sample sizes in nonregulated health care intervention trials described as randomized controlled trials were overly similar.” Journal of Clinical Epidemiology, 120, p. 8-16. doi:10.1016/j.jclinepi.2019.12.011. Thombs, B.D., Kwakkenbos, L., Levis, A.W., Benedetti, A., 2018. “Addressing overestimation of the prevalence of depression based on self-report screening questionnaires.” Canadian Medical Association Journal, 190, p. E44-E49. doi:10.1503/cmaj.170691. Coronado-Montoya, S., Levis, A.W., Kwakkenbos, L., Steele, R.J., et al., 2016. “Reporting of positive results in randomized controlled trials of mindfulness-based mental health interventions.” PloS One, 11(4):e0153220. doi:10.1371/journal.pone.0153220. Levis, A.W., Leentjens, A.F., Levenson, J.L., Lumley, M.A., et al., 2015. “Comparison of self-citation by peer reviewers in a journal with single-blind peer review versus a journal with open peer review.” Journal of Psychosomatic Research, 79(6), p. 561-565. doi:10.1016/j.jpsychores.2015.08.004. Thombs, B.D., Levis, A.W., Razykov, I., Syamchandra, A., et al., 2015. “Potentially coercive self-citation by peer reviewers: a cross-sectional study.” Journal of Psychosomatic Research, 78(1), p. 1-6. doi:10.1016/j.jpsychores.2014.09.015. ","date":"2024-08-03","objectID":"/research/:8:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Item Response Theory Harel, D., Levis, B., Ishihara, M., Levis, A.W., et al., 2021. “Shortening the Edinburgh postnatal depression scale using optimal test assembly methods: Development of the EPDS‐Dep‐5”. Acta Psychiatrica Scandinavica, 143(4), p. 348-362. doi:10.1111/acps.13272. Ishihara, M., Harel, D., Levis, B., Levis, A.W., et al., 2019. “Shortening self‐report mental health symptom measures through optimal test assembly methods: Development and validation of the Patient Health Questionnaire‐Depression‐4.” Depression and Anxiety, 36(1), p. 82-92. doi:10.1002/da.22841. Levis, A.W., Harel, D., Kwakkenbos, L., Carrier, M.E., et al., 2016. “Using optimal test assembly methods for shortening patient-reported outcome measures: Development and validation of the Cochin Hand Function Scale-6: A scleroderma patient-centered intervention network cohort study.” Arthritis Care \u0026 Research, 68(11), p. 1704-1713. doi:10.1002/acr.22893. ","date":"2024-08-03","objectID":"/research/:9:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Systematic Reviews and Meta-analysis Delisle, V.C., Gumuchian, S.T., Rice, D.B., Levis, A.W., et al., 2017. “Perceived benefits and factors that influence the ability to establish and maintain patient support groups in rare diseases: a scoping review.” The Patient-Patient-Centered Outcomes Research, 10, p. 283-293. doi:10.1007/s40271-016-0213-9. Levis, B., Benedetti, A., Levis, A.W., Ioannidis, J.P., et al., 2017. “Selective cutoff reporting in studies of diagnostic test accuracy: a comparison of conventional and individual-patient-data meta-analyses of the Patient Health Questionnaire-9 depression screening tool.” Americal Journal of Epidemiology, 185(10), p. 954-964. doi:10.1093/aje/kww191. ","date":"2024-08-03","objectID":"/research/:10:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"Nutrition and Global Health Golden, C.D., Zamborain-Mason, J., Levis, A., Rice, B.L., et al., 2024. “Prevalence of micronutrient deficiencies across diverse environments in rural Madagascar.” Frontiers in Nutrition, 11, p. 1389080. doi:10.3389/fnut.2024.1389080. Truche, P., Botelho, F., Bowder, A.N., Levis, A.W., et al., 2021. “Potentially avertable child mortality associated with surgical workforce scale-up in low-and middle-income countries: a global study.” World Journal of Surgery, 45(9), p. 2643-2652. doi:10.1007/s00268-021-06181-6. * Denotes equal author contribution ","date":"2024-08-03","objectID":"/research/:11:0","tags":null,"title":"Research Areas and Selected Publications","uri":"/research/"},{"categories":null,"content":"teaching history and materials","date":"2024-06-16","objectID":"/teaching/","tags":null,"title":"Teaching","uri":"/teaching/"},{"categories":null,"content":"Harvard University—Teaching Assistantships ","date":"2024-06-16","objectID":"/teaching/:0:0","tags":null,"title":"Teaching","uri":"/teaching/"},{"categories":null,"content":"Theory and Methods for Causality II — BST 257 (Fall, 2021) Below are some original notes I prepared for course labs. Projections in Hilbert Spaces Projections in Finite Dimensions \u0026 Least Squares Error Expansions in Statistics Delta Method ","date":"2024-06-16","objectID":"/teaching/:1:0","tags":null,"title":"Teaching","uri":"/teaching/"},{"categories":null,"content":"Statistical Inference I — BST 231 (Spring, 2020) ","date":"2024-06-16","objectID":"/teaching/:2:0","tags":null,"title":"Teaching","uri":"/teaching/"},{"categories":null,"content":"Advanced Regression \u0026 Statistical Learning — BST 235 (Fall, 2019) See here for notes on Weyl’s inequality, and below for all lab materials I prepared for the course. Lab 1: Vector Spaces (Solutions) Lab 2: Linear Maps and Matrices (Solutions) Lab 3: Projections and Random Vectors (Solutions) Lab 4: Population and Sample Least Squares (Solutions) Lab 5: Spectral Theory and Fisher-Cochran (Solutions) Lab 6: General Linear and Subspace Testing (Solutions) Lab 7: Identifiability and Asymptotics (Solutions) ","date":"2024-06-16","objectID":"/teaching/:3:0","tags":null,"title":"Teaching","uri":"/teaching/"},{"categories":null,"content":"Core Principles of Biostatistics and Epidemiology for Public Health Practice — ID 201 (Fall, 2018) ","date":"2024-06-16","objectID":"/teaching/:4:0","tags":null,"title":"Teaching","uri":"/teaching/"},{"categories":null,"content":"Recently, I’ve been listening to (i.e., binging) the Casual Inference podcast hosted by Lucy D’Agostino McGowan and Ellie Murrary, and a question was raised at least a couple of times (Season 3 Episode 10 \u0026 Season 4 Episode 9): does adjusting for an instrument (in, e.g., a propensity score model) cause bias? I believe the answer to this question is no, except in somewhat extreme scenarios. However, such a choice will impact the efficiency of a treatment effect estimator. In this post, I want to review a pair of lemmas from an excellent paper by Rotnitzky \u0026 Smucler (2020; JMLR) to formally address this and related issues. ","date":"2024-04-29","objectID":"/instrument-precision/:0:0","tags":null,"title":"To adjust or not to adjust: instruments and precision variables","uri":"/instrument-precision/"},{"categories":null,"content":"Graphs, Instruments, and Precision Variables Say we are interested in the effect of an exposure, \\(A\\), on an outcome, \\(Y\\). In order to use to data to estimate the causal effect of \\(A\\) on \\(Y\\), we typically aim to adjust for a sufficient set of confounders, $\\boldsymbol{L}$—informally, \\(\\boldsymbol{L}\\) are the common causes of \\(A\\) on \\(Y\\). Graphically, we can represent these relationships on a causal directed acyclic graph (DAG): Figure 1: Canonical observational study DAG The DAG in Figure 1 encodes the assumption that, within levels of \\(\\boldsymbol{L}\\), the treatment is as good as randomized. In counterfactual language, we would say that \\begin{equation} A \\perp\\!\\!\\!\\!\\perp Y(a) \\mid \\boldsymbol{L}, \\end{equation} where \\(Y(a)\\) is the potential outcome that would occur under exposure level \\(A = a\\). Graphically, \\(\\boldsymbol{L}\\) blocks all backdoor paths (i.e., those starting with an edge into the treatment, \\(A \\leftarrow\\)) from \\(A\\) to \\(Y\\). [ Aside: the connection between the DAG and potential outcomes is not obvious, and requires one to associate a structural model with the DAG. With the FFRCISTG model, one can read off counterfactual independence statements like this using single world intervention graphs—this is a discussion for another day! ] In a given scientific setting, the situation may be more complicated than that in the above picture. That is, we might imagine a more refined DAG with a greater number of pre-treatment variables depicted and certain arrows missing due to domain knowledge. Instruments and precision variables represent two special cases of such additional variables. An instrument, simply put, is a pure predictor of the exposure. In this context, we will say \\(Z\\) is an instrument if it causes \\(A\\) but only has an effect on \\(Y\\) through \\(A\\): in DAG form, Figure 2: Observational study with an instrument Note that I use the two-sided arrow (\\(\\leftrightarrow\\)) between \\(\\boldsymbol{L}\\) and \\(Z\\), as the direction of association (or indeed the existence of an exogenous common cause) does not affect our discussion. A precision variable, on the other hand, is a pure predictor of the outcome. That is, \\(W\\) is a precision variable if it directly affects \\(Y\\), but is only associated with \\(A\\) through associations with \\(\\boldsymbol{L}\\): Figure 3: Observational study with a precision variable In these alternative scenarios, we are left with multiple valid adjustment sets. That is, in the DAG in Figure 2, we will have \\((1)\\) in addition to \\(A \\perp\\!\\!\\!\\!\\perp Y(a) \\mid \\boldsymbol{L}, Z\\). Meanwhile, in the DAG in Figure 3, we have \\((1)\\) in addition to \\(A \\perp\\!\\!\\!\\!\\perp Y(a) \\mid \\boldsymbol{L}, W\\). In each case, we would like to know: should we adjust only for \\(\\boldsymbol{L}\\), or should we include the additional variable as well? ","date":"2024-04-29","objectID":"/instrument-precision/:1:0","tags":null,"title":"To adjust or not to adjust: instruments and precision variables","uri":"/instrument-precision/"},{"categories":null,"content":"The AIPW estimating function and its variance In order to make the above question concrete, we need to specify at the very least (i) a precise causal estimand of interest, (ii) one or more identification formulas for this estimand, and (iii) an estimator for these functionals. We will assume that we observe a random sample \\(O_1, \\ldots, O_n \\overset{\\mathrm{iid}}{\\sim} \\mathbb{P}\\), where a typical observation is \\(O = (\\boldsymbol{L}, A, Y)\\) or \\((\\boldsymbol{L}, Z, A, Y)\\) or \\((\\boldsymbol{L}, W, A, Y)\\), for the three scenarios above, respectively. The estimand of interest, for our purposes, will be the population average treatment effect (ATE), $\\mathbb{E}(Y(1) - Y(0))$—we will assume for simplicity that the exposure is binary, \\(A \\in \\{0,1\\}\\). To proceed with identification (and consequently to have a hope at estimation), we need one crucial positivity assumption. Assumption (\\(*\\)): For any \\(\\boldsymbol{B} \\subseteq O \\setminus \\{A, Y\\}\\), \\(\\mathbb{P}[A = 1 \\mid \\boldsymbol{B}] \\in (0,1)\\) with probability 1. In all three DAG scenarios presented above, and under Assumption (\\(*\\)), the ATE is identified by the statistical functional \\[\\chi(\\mathbb{P}) = \\mathbb{E}_{\\mathbb{P}}(\\mu_1(\\boldsymbol{L}) - \\mu_0(\\boldsymbol{L})),\\] where \\(\\mu_a(\\boldsymbol{L}) = \\mathbb{E}_{\\mathbb{P}}(Y \\mid \\boldsymbol{L}, A = a)\\); henceforth we will suppress dependence on \\(\\mathbb{P}\\) and often omit inputs to functions when there is no ambiguity. In order to represent alternative adjustment sets, we will augment our notation: define \\[\\chi_{\\boldsymbol{B}} = \\mathbb{E}(\\mu_{1, \\boldsymbol{B}} - \\mu_{0, \\boldsymbol{B}}),\\] where \\(\\boldsymbol{B} \\subseteq O \\setminus \\{A, Y\\}\\), and \\(\\mu_{a, \\boldsymbol{B}}(\\boldsymbol{B}) \\equiv \\mathbb{E}(Y \\mid \\boldsymbol{B}, A = a)\\). The ATE, \\(\\mathbb{E}(Y(1) - Y(0))\\), equals to \\(\\chi_{\\boldsymbol{B}}\\) where \\(\\boldsymbol{B} = \\boldsymbol{L}\\) in the DAG of Figure 1, \\(\\boldsymbol{B} = \\boldsymbol{L}\\) or \\((\\boldsymbol{L}, Z)\\) in the DAG of Figure 2, and \\(\\boldsymbol{B} = \\boldsymbol{L}\\) or \\((\\boldsymbol{L}, W)\\) in the DAG of Figure 3. With identification out of the way, we are left with the statistical task of estimating these quantities. I tend to prefer, when possible, estimators that are asymptotically “optimal”. In particular, estimators based on influence functions have really nice theoretical properties. In a nonparametric model, a given functional has a unique influence function. That said, the presence of an instrument or precision variable means that the statistical model is no longer completely nonparametric, and is actually a proper semiparametric model. Concretely, the absence of arrows in a DAG implies conditional independence restrictions on the observed data distribution \\(\\mathbb{P}\\): in the DAG of Figure 2, \\(Z \\perp\\!\\!\\!\\!\\perp Y \\mid \\boldsymbol{L}, A\\), while in the DAG of Figure 3, \\(A \\perp\\!\\!\\!\\!\\perp W \\mid \\boldsymbol{L}\\). As a consequence, there are infinitely many influence functions one may work with, and the best choice—the efficient influence function (EIF)—is that with the lowest variance. The variance of a functional’s EIF represents a local asymptotic minimax lower bound for estimation of that functional, and thus one often aims to construct an estimator that attains this variance bound asymptotically. Our goals in this note are a tad more modest, and we will instead consider estimators based solely on the nonparametric influence functions of \\(\\chi_{\\boldsymbol{B}}\\) for different choices of \\(\\boldsymbol{B}\\). Let \\(\\pi_{a, \\boldsymbol{B}}(\\boldsymbol{B}) = \\mathbb{P}[A = a \\mid \\boldsymbol{B}]\\) be the exposure probabilities on the basis of variables \\(\\boldsymbol{B}\\), and define \\begin{equation} \\phi_{\\boldsymbol{B}} = \\mu_{1, \\boldsymbol{B}} - \\mu_{0, \\boldsymbol{B}} + \\frac{2A - 1}{\\pi_{A, \\boldsymbol{B}}}(Y - \\mu_{A, \\boldsymbol{B}}), \\end{equation} which is the (uncentered) nonparametric influence function of the functional \\(\\chi_{\\b","date":"2024-04-29","objectID":"/instrument-precision/:2:0","tags":null,"title":"To adjust or not to adjust: instruments and precision variables","uri":"/instrument-precision/"},{"categories":null,"content":"Do not adjust for instruments! Lemma 2: (Adapted from Lemma 5 in Rotnitzky \u0026 Smucler (2020)) Suppose \\(\\boldsymbol{Z} \\perp\\!\\!\\!\\!\\perp Y \\mid \\boldsymbol{L}, A\\). Then \\(\\mathrm{Var}(\\phi_{\\boldsymbol{L}, \\boldsymbol{Z}}) \\geq \\mathrm{Var}(\\phi_{\\boldsymbol{L}})\\). Proof: Note that under the stated conditional independence assumption, \\(\\mu_{a,\\boldsymbol{L}, \\boldsymbol{Z}} \\equiv \\mu_{a,\\boldsymbol{L}}\\) and \\(\\sigma_{a, \\boldsymbol{L}, \\boldsymbol{Z}}^2 \\equiv \\sigma_{a, \\boldsymbol{L}}^2\\). We therefore have \\[\\mathbb{E}(\\phi_{\\boldsymbol{L}, \\boldsymbol{Z}} \\mid \\boldsymbol{L}, A, Y) = \\mu_{1, \\boldsymbol{L}} - \\mu_{0, \\boldsymbol{L}} + (2A - 1)(Y - \\mu_{A, \\boldsymbol{L}})\\mathbb{E}\\left(\\frac{1}{\\pi_{A, \\boldsymbol{L}, \\boldsymbol{Z}}} \\mid \\boldsymbol{L}, A \\right),\\] again using the fact that \\(\\boldsymbol{Z} \\perp\\!\\!\\!\\!\\perp Y \\mid \\boldsymbol{L}, A\\). Using Lemma 3 below, we then have \\(\\mathbb{E}(\\phi_{\\boldsymbol{L}, \\boldsymbol{Z}} \\mid \\boldsymbol{L}, A, Y) = \\phi_{\\boldsymbol{L}}\\). Thus, by the law of total variance, \\[\\mathrm{Var}\\left(\\phi_{\\boldsymbol{L}, \\boldsymbol{Z}}\\right) = \\mathrm{Var}\\left(\\phi_{\\boldsymbol{L}}\\right) + \\mathbb{E}\\left(\\mathrm{Var}(\\phi_{\\boldsymbol{L}, \\boldsymbol{Z}} \\mid \\boldsymbol{L}, A, Y)\\right) \\geq \\mathrm{Var}\\left(\\phi_{\\boldsymbol{L}}\\right).\\] Explicitly, if you like, \\[\\mathbb{E}\\left(\\mathrm{Var}( \\phi_{\\boldsymbol{L}, \\boldsymbol{Z}} \\mid \\boldsymbol{L}, A, Y)\\right) = \\mathbb{E}\\left(\\sigma_{A, \\boldsymbol{L}}^2 \\cdot \\mathrm{Var}\\left(\\frac{1}{\\pi_{A, \\boldsymbol{L}, \\boldsymbol{Z}}} \\mid \\boldsymbol{L}, A\\right)\\right),\\] which is certainly non-negative. \\(\\quad \\blacksquare\\) Lemma 3: \\(\\mathbb{E}\\left(\\frac{1}{\\pi_{A, \\boldsymbol{L}, \\boldsymbol{Z}}} \\mid \\boldsymbol{L}, A \\right) = \\frac{1}{\\pi_{A, \\boldsymbol{L}}}\\). Proof: For each \\(a \\in \\{0,1\\}\\), observe that \\begin{align*} \\mathbb{E}\\left(\\frac{1}{\\pi_{a, \\boldsymbol{L}, \\boldsymbol{Z}}} \\mid \\boldsymbol{L}, A = a\\right)\\pi_{a, \\boldsymbol{L}} \u0026= \\mathbb{E}\\left(\\frac{I(A = a)}{\\pi_{a, \\boldsymbol{L}, \\boldsymbol{Z}}} \\mid \\boldsymbol{L}\\right) \\\\ \u0026= \\mathbb{E}\\left(\\frac{\\mathbb{E}(I(A = a) \\mid \\boldsymbol{L}, \\boldsymbol{Z}) }{\\pi_{a, \\boldsymbol{L}, \\boldsymbol{Z}}} \\mid \\boldsymbol{L}\\right) \\\\ \u0026= 1, \\end{align*} which proves the result. \\(\\quad \\blacksquare\\) ","date":"2024-04-29","objectID":"/instrument-precision/:3:0","tags":null,"title":"To adjust or not to adjust: instruments and precision variables","uri":"/instrument-precision/"},{"categories":null,"content":"Do adjust for precision variables! Lemma 4: (Adapted from Lemma 4 in Rotnitzky \u0026 Smucler (2020)) Suppose \\(A \\perp\\!\\!\\!\\!\\perp \\boldsymbol{W} \\mid \\boldsymbol{L}\\). Then \\(\\mathrm{Var}(\\phi_{\\boldsymbol{L}, \\boldsymbol{W}}) \\leq \\mathrm{Var}(\\phi_{\\boldsymbol{L}})\\). Proof: Note that under the stated conditional independence assumption, \\(\\pi_{A,\\boldsymbol{L}, \\boldsymbol{W}} \\equiv \\pi_{A,\\boldsymbol{L}}\\). Thus, \\begin{align*} \\phi_{\\boldsymbol{L}} \u0026= \\phi_{\\boldsymbol{L}, \\boldsymbol{W}} + \\left(\\frac{A}{\\pi_{1, \\boldsymbol{L}}} - 1\\right) (\\mu_{1, \\boldsymbol{L}, \\boldsymbol{W}} - \\mu_{1, \\boldsymbol{L}}) - \\left(\\frac{1 - A}{1 - \\pi_{1, \\boldsymbol{L}}} - 1\\right) (\\mu_{0, \\boldsymbol{L}, \\boldsymbol{W}} - \\mu_{0, \\boldsymbol{L}}) \\end{align*} Noting that, as \\(A \\perp\\!\\!\\!\\!\\perp \\boldsymbol{W} \\mid \\boldsymbol{L}\\), \\[\\mathbb{E}\\left(\\left(\\frac{A}{\\pi_{1, \\boldsymbol{L}}} - 1\\right) (\\mu_{1, \\boldsymbol{L}, \\boldsymbol{W}} - \\mu_{1, \\boldsymbol{L}}) - \\left(\\frac{1 - A}{1 - \\pi_{1, \\boldsymbol{L}}} - 1\\right) (\\mu_{0, \\boldsymbol{L}, \\boldsymbol{W}} - \\mu_{0, \\boldsymbol{L}}) \\mid \\boldsymbol{L}, \\boldsymbol{W}\\right) = 0,\\] it then follows that \\[\\mathrm{Cov}\\left(\\phi_{\\boldsymbol{L}, \\boldsymbol{W}}, \\left(\\frac{A}{\\pi_{1, \\boldsymbol{L}}} - 1\\right) (\\mu_{1, \\boldsymbol{L}, \\boldsymbol{W}} - \\mu_{1, \\boldsymbol{L}}) - \\left(\\frac{1 - A}{1 - \\pi_{1, \\boldsymbol{L}}} - 1\\right) (\\mu_{0, \\boldsymbol{L}, \\boldsymbol{W}} - \\mu_{0, \\boldsymbol{L}})\\right) = 0.\\] Thus, we have \\begin{align*} \u0026 \\mathrm{Var}(\\phi_{\\boldsymbol{L}}) \\\\ \u0026= \\mathrm{Var}(\\phi_{\\boldsymbol{L}, \\boldsymbol{W}}) + \\mathrm{Var}\\left(\\left(\\frac{A}{\\pi_{1, \\boldsymbol{L}}} - 1\\right)(\\mu_{1, \\boldsymbol{L}, \\boldsymbol{W}} - \\mu_{1, \\boldsymbol{L}}) - \\left(\\frac{1 - A}{1 - \\pi_{1, \\boldsymbol{L}}} - 1\\right) (\\mu_{0, \\boldsymbol{L}, \\boldsymbol{W}} - \\mu_{0, \\boldsymbol{L}})\\right) \\\\ \u0026 \\geq \\mathrm{Var}(\\phi_{\\boldsymbol{L}, \\boldsymbol{W}}), \\end{align*} as variance is non-negative. \\(\\quad \\blacksquare\\) ","date":"2024-04-29","objectID":"/instrument-precision/:4:0","tags":null,"title":"To adjust or not to adjust: instruments and precision variables","uri":"/instrument-precision/"},{"categories":null,"content":"Perfect predicition of treatment Our discussion thus far has relied on positivity for any given adjustment set \\(\\boldsymbol{B}\\), i.e., Assumption (\\(*\\)). What if this assumption holds for \\(\\boldsymbol{B} = \\boldsymbol{L}\\), but is violated for \\(\\boldsymbol{B} = (\\boldsymbol{L}, \\boldsymbol{Z})\\)? That is, what if we include an instrument (or set of instruments) that results in perfect prediction of treatment status for some subgroup: \\[\\mathbb{P}\\left[\\mathbb{P}[A = 1 \\mid \\boldsymbol{L}, \\boldsymbol{Z}] \\in \\{0,1\\}\\right] \u003e 0.\\] Well, unfortunately identification breaks down because one of \\(\\mu_{1, \\boldsymbol{L}, \\boldsymbol{Z}}\\) or \\(\\mu_{0, \\boldsymbol{L}, \\boldsymbol{Z}}\\) will not be well-defined with some positive probability, therefore \\(\\chi_{\\boldsymbol{L}, \\boldsymbol{Z}}\\) will not be well-defined. More realistically, in my opinion, inclusion of very predictive instruments may result in practical near-positivity violations, i.e., \\(\\mathbb{P}[A = 1 \\mid \\boldsymbol{L}, \\boldsymbol{Z}]\\) may be very close to 0 or 1. If this occurs, our outcome model estimates can become unstable when there is little data for one treatment level, and moreover the asymptotic variance of \\(\\widehat{\\chi}_{\\boldsymbol{L}, \\boldsymbol{Z}}\\), \\(\\mathrm{Var}(\\phi_{\\boldsymbol{L}, \\boldsymbol{Z}})\\), may explode—as a fun exercise, inspect the variance formulas in Lemmas 1 and 2 to get a feel for where exactly this manifests. ","date":"2024-04-29","objectID":"/instrument-precision/:5:0","tags":null,"title":"To adjust or not to adjust: instruments and precision variables","uri":"/instrument-precision/"},{"categories":null,"content":"Practical takeaways In practice, one may not be sure a priori whether a given variable is an exact instrument or precision variable. What should one do in such cases? For putative precision variables (e.g., those thought to be strongly predictive of the outcome, but maybe only weakly predictive of exposure), it is clear that these should definitely be measured if possible and included in an adjustment set. For putative instruments, perhaps one should err on the side of ensuring a valid analysis and include covariates that may only be weakly associated with the outcome (after controlling for \\(\\boldsymbol{L}\\) and \\(A\\)). When one is absolutely sure on the basis of domain knowledge, an instrument can be excluded to gain some efficiency. Given the discussion in the last section, I would emphasize that care should be taken when including very strong predictors of treatment (unless they are also very strong predictors of the outcome, whereby we would be forced to include them as confounders). More broadly, the discussion in this note assumes we are in the luxurious setting where we measure a sufficient set of confounders of the exposure-outcome relationship. Often, one questions whether this is even the case, and it is typically possible to posit some important unmeasured confounders. If one believes they have an instrument, and that it is more likely to be unconfounded than the exposure itself, then one may be able to exploit this structure for an alternative route towards partial or full identification of the exposure effect—if you’re interested, you might enjoy our work on partial identification and our review paper on identification using instrumental variables! For a deeper dive into the issues discussed in this note, the consideration of arbitrarily complicated DAGs, the extension to time-varying treatments, and much more, see the paper by Rotnitzky \u0026 Smucler. ","date":"2024-04-29","objectID":"/instrument-precision/:6:0","tags":null,"title":"To adjust or not to adjust: instruments and precision variables","uri":"/instrument-precision/"},{"categories":null,"content":"I got stuck on what I thought was a simple point while working through a stats textbook! I figured it out, I think, but was this the most straightforward path to understanding? You tell me! In this note, we explore some oft-used facts about moment-generating functions, and explain a connection with tail and concentration inequalities. ","date":"2022-08-05","objectID":"/mgf-bounds/:0:0","tags":null,"title":"MGFs, power series, and pushing the expectation through","uri":"/mgf-bounds/"},{"categories":null,"content":"Overview and my confusion I am currently reading through Martin Wainwright’s High-dimensional statistics: A non-asymptotic viewpoint, to fill some gaps in my stat theory knowledge. While I’m only partway through, I can definitely already recommend this book for those wanting to get familiar with some of the modern theoretical machinery needed to validate statistical methods that we use on difficult problems! The book starts off relatively lightly in Chapter 2 with an overview of fundamental tail and concentration bounds. A simple but powerful idea is that of Chernoff bounding, which combines the moment generating function of a random variable (when it exists), and Markov’s inequality to tightly bound tail probabilities. In reading through this chapter, one quickly comes across the sub-exponential class of distributions: a random variable \\(X\\) is sub-exponential if \\(\\mathbb{E}(|X|) \u003c \\infty\\) and there exist constants \\(\\nu \u003e 0\\) and \\(\\alpha \\in [0,\\infty)\\) such that \\[\\mathbb{E}(e^{t (X-\\mu)}) \\leq e^{\\frac{\\nu^2 t^2}{2}}, \\text{for all } t \\in \\left(-\\frac{1}{\\alpha}, \\frac{1}{\\alpha}\\right),\\] where \\(\\mu \\coloneqq \\mathbb{E}(X)\\) and we use the convention that \\(\\frac{1}{0} = \\infty\\). Following this introduction, we are then exposed to a sufficient condition for \\(X\\) to be sub-exponential, the so-called Bernstein condition. In particular, \\(X\\) is said to be $b$-Bernstein, for some \\(b \u003e 0\\), if \\(\\mathbb{E}(|X|^2) \u003c \\infty\\), and letting \\(\\mu \\coloneqq \\mathbb{E}(X)\\), \\(\\sigma^2 \\coloneqq \\mathrm{Var}(X)\\), it holds that \\[\\left|\\mathbb{E}[(X - \\mu)^k]\\right| \\leq \\frac{1}{2}k! \\sigma^2 b^{k-2}, \\text{ for all } k \\in \\mathbb{N} \\setminus \\{1\\}.\\] In proving that this condition implies that \\(X\\) is sub-exponential, the author employs the “power series decomposition” \\(\\mathbb{E}(e^{t(X- \\mu)}) = \\sum_{n = 0}^\\infty \\frac{t^n}{n!} \\mathbb{E}[(X-\\mu)^n]\\), for \\(t\\) non-zero but sufficiently small. That said, I couldn’t find anywhere in the book where this equality was justified. At first glance, this equality might seem obvious, as \\(e^{z} = \\sum_{n=0}^\\infty \\frac{z^n}{n!}\\) for all \\(z \\in \\mathbb{R}\\), so one should just be able to push the expectation through this power series representation. But usually this sort of statement is justified by the dominated convergence theorem (DCT): if \\(Y_n \\to Y\\), and the sequence \\(Y_n\\) is bounded by \\(Z \\in L_1\\), i.e., \\(|Y_n| \\leq Z\\) for all \\(n \\in \\mathbb{N}\\) and \\(\\mathbb{E}(Z) \u003c \\infty\\), then \\(\\mathbb{E}(|Y_n|), \\mathbb{E}(|Y|) \u003c \\infty\\) and \\(\\mathbb{E}(Y) = \\lim_{n \\to \\infty} \\mathbb{E}(Y_n)\\). In our problem, can we just directly apply the DCT? Well, we would like to say that, for small but non-zero \\(t\\), \\[\\mathbb{E}\\left(\\lim_{n \\to \\infty}\\sum_{k=0}^n \\frac{t^k}{k!}(X-\\mu)^k\\right) = \\lim_{n \\to \\infty}\\mathbb{E}\\left(\\sum_{k=0}^n \\frac{t^k}{k!}(X-\\mu)^k\\right),\\] i.e., \\(Y_n = \\sum_{k=0}^n \\frac{t^k}{k!}(X-\\mu)^k\\) and \\(Y = e^{t(X-\\mu)}\\). But what should be the dominating variable \\(Z\\)? We can easily see that \\(Y_n \\leq \\sum_{k=0}^n \\frac{|t|^n}{n!} |X-\\mu|^n \\leq e^{|t| |X-\\mu|}\\), but do we know that the latter is integrable? It wasn’t obvious to me, so let’s dive into the details. ","date":"2022-08-05","objectID":"/mgf-bounds/:1:0","tags":null,"title":"MGFs, power series, and pushing the expectation through","uri":"/mgf-bounds/"},{"categories":null,"content":"Facts about moment generating functions Let \\(X\\) be a random variable, and define \\(\\Psi_X(t) = \\mathbb{E}(e^{tX})\\), for \\(t \\in D\\), where \\(D = \\{t \\in \\mathbb{R}: \\mathbb{E}(e^{tX}) \u003c \\infty\\}\\). Note that \\(e^{tX} \\geq 0\\), so that its expectation is always well-defined in \\([0, \\infty]\\). Moreover, \\(0 \\in D\\) no matter what, as \\(\\mathbb{E}(e^{0X}) = 1 \u003c \\infty\\) — this could possibly be the only element of \\(D\\). The first interesting fact about \\(\\Psi_X\\) is that its domain \\(D\\) is always an interval including zero. Lemma 1: If \\(t \\in D \\cap (0, \\infty)\\), then \\([0, t] \\subseteq D\\). Similarly, if \\(t \\in D \\cap (-\\infty, 0)\\), then \\([t, 0] \\subseteq D\\). Proof: Suppose \\(t \u003e 0\\), \\(\\mathbb{E}(e^{tX}) \u003c \\infty\\), and let \\(s \\in [0, t]\\). Observe that \\[\\mathbb{E}(e^{sX}) = \\mathbb{E}(e^{sX} \\mathbf{1}_{[0, \\infty)}(X)) + \\mathbb{E}(e^{sX} \\mathbf{1}_{(-\\infty, 0)}(X)) \\leq \\mathbb{E}(e^{tX} \\mathbf{1}_{[0,\\infty)}) + P[X \u003c 0] \\leq \\Psi_{X}(t) + 1 \u003c \\infty,\\] so \\(s \\in D\\). Similarly, if \\(t \u003c 0\\), \\(\\mathbb{E}(e^{tX}) \u003c \\infty\\), then let \\(s \\in [t, 0]\\) and note that \\[\\mathbb{E}(e^{sX}) = \\mathbb{E}(e^{sX} \\mathbf{1}_{(0, \\infty)}(X)) + \\mathbb{E}(e^{sX} \\mathbf{1}_{(-\\infty, 0]}(X)) \\leq P[X \u003e 0] + \\mathbb{E}(e^{tX} \\mathbf{1}_{(-\\infty, 0]}(X)) \\leq 1 + \\Psi_X(t),\\] so \\(s \\in D\\). \\(\\quad \\blacksquare\\) MGFs are useful when they exist on an open interval around zero — it is in this case that they actually generate moments! This is formalized in the next result. Proposition 1: Suppose \\(t^* \u003e 0\\) is such that \\((-t^*, t^*) \\subseteq D\\). Then \\(\\mathbb{E}(|X|^n) \u003c \\infty\\), for all \\(n \\in \\mathbb{N}\\), \\[\\Psi_X(t) = \\sum_{n = 0}^\\infty \\frac{t^n}{n!} \\mathbb{E}(X^n)\\] holds and this series is absolutely convergent, for all \\(t \\in (-t^*, t^*)\\). Consequently, \\(\\mathbb{E}(X^n) = \\left. \\frac{d^n}{dt^n} \\Psi_X(t) \\right|_{t = 0}\\). Proof: Let \\(t \\in (0, t^*)\\). The key observation is that \\(e^{t|x|} \\leq e^{-tx} + e^{tx}\\), for all \\(x \\in \\mathbb{R}\\). Thus, \\[\\mathbb{E}(e^{t|X|}) \\leq \\Psi_X(-t) + \\Psi_X(t) \u003c \\infty,\\] given that \\(-t, t \\in D\\). In other words, by the power series expansion of the exponential, \\[\\mathbb{E}(e^{t|X|}) = \\mathbb{E}\\left(\\sum_{n=0}^{\\infty} \\frac{t^n}{n!} |X|^n\\right) = \\sum_{n=0}^{\\infty} \\frac{t^n}{n!} \\mathbb{E}(|X|^n) \u003c \\infty,\\] where the interchange of expectation and summation is permitted by the monotone convergence theorem (MCT). Immediately, we can deduce that \\(\\mathbb{E}(|X|^n) \u003c \\infty\\) for all \\(n \\in \\mathbb{N}\\). We further can see by the DCT that \\(\\Psi_X(t) = \\sum_{n=0}^\\infty \\frac{t^n}{n!} \\mathbb{E}(X^n)\\) for any \\(t \\in (-t^*, t^*)\\), as the partial sums \\(\\sum_{k=0}^N \\frac{t^n}{n!} X^n\\), for \\(N \\in \\mathbb{N}\\), are uniformly bounded by \\(e^{|t| \\cdot |X|} \\in L_1\\). Notice that we have shown that the power series \\(\\sum_{n=0}^{\\infty} \\frac{t^n}{n!} \\mathbb{E}(X^n)\\) converges absolutely on \\((-t^*, t^*)\\), i.e., its radius of convergence is at least \\(t^*\\). By basic facts about power series, \\(\\Psi_X(t)\\) is infinitely differentiable on \\((-t^*, t^*)\\), and \\(\\mathbb{E}(X^n) = \\left. \\frac{d^n}{dt^n} \\Psi_X(t) \\right|_{t = 0}\\). \\(\\quad \\blacksquare\\) Inspecting the proof of Proposition 1, we can see that the real power (no pun intended) comes from \\(e^{|tX|}\\) being integrable. The crux of the argument was that \\(\\{-t, t\\} \\subseteq D\\) is equivalent to \\(\\mathbb{E}(e^{|tX|}) \u003c \\infty\\), and the latter readily establishes that \\(\\Psi_X\\) permits the power series representation \\(\\Psi_X(s) = \\sum_{n=0}^\\infty \\frac{s^n}{n!}\\mathbb{E}(X^n)\\), for \\(s \\in \\{-t, t\\}\\). With Lemma 1, we pretty much immediately obtain the following nice chain of equivalences. Theorem: For any \\(t \\in \\mathbb{R}\\): \\begin{align*} \u0026\\{-t, t\\} \\subseteq D \\text{ (or say } [-|t|, |t|] \\subseteq D \\text{ if you like)}\\\\ \\iff \u0026\\mathbb{E}(e^{|sX|}) \u003c \\infty, \\textit{for all } s \\in [-|t|, |t|] \\\\ \\iff \u0026\\sum_{n=0}^\\infty \\frac{t^n}{n!} \\mathbb{E}(X^n) \\textit{ absolutely c","date":"2022-08-05","objectID":"/mgf-bounds/:2:0","tags":null,"title":"MGFs, power series, and pushing the expectation through","uri":"/mgf-bounds/"},{"categories":null,"content":"Relation to sub-exponential distributions At this point, we are equipped to understand a more general version of the problem we started out with (side note: are the necessary facts about MGFs just supposed to be common knowledge!?). Namely, we will verify that \\(X\\) is sub-exponential if and only if the domain \\(D\\) of \\(\\Psi_X\\) contains an open interval around zero [for those reading the Wainwright text, this is part of Theorem 2.13]. Clearly, if \\(\\nu \u003e 0\\) and \\(\\alpha \\in [0,\\infty)\\) satisfy \\(\\mathbb{E}(e^{t(X-\\mu)}) \\leq e^{\\frac{\\nu^2 t^2}{2}}\\) (notice in particular that this is finite) for \\(t \\in (-\\alpha^{-1}, \\alpha^{-1})\\), then \\(D\\) contains any pair \\(\\{-t, t\\}\\) for \\(t \\in (0, \\alpha^{-1})\\), and we see that \\((-\\alpha^{-1}, \\alpha^{-1}) \\subseteq D\\). The following says that the converse result also holds. Proposition 2: The MGF of \\(X\\) is defined on \\(D \\supseteq (-c, c)\\) for some \\(c \u003e 0\\) if and only if \\(X\\) is sub-exponential. Proof: We have just argued in the “if” direction. For the converse, suppose \\(c \u003e 0\\) and \\((-c,c) \\subseteq D\\). By our theorem, it is kosher to use the power series expansion of \\(\\Psi_{X - \\mu}(t)\\) for any \\(t \\in (-c,c)\\) — note by the way that the domain \\(D\\) of \\(\\Psi_X\\) is equal to that of the MGF of the centered variable \\(\\Psi_{X - \\mu}\\), as they differ by a deterministic finite multiplicative factor \\(e^{-t\\mu}\\). Further, as the radius of convergence of this power series is at least \\(c\\), we can differentiate term by term on \\((-c,c)\\) to obtain the derivative of \\(\\Psi_{X - \\mu}\\). Consequently, by Taylor’s theorem with Peano remainder, \\[\\Psi_{X - \\mu}(t) = \\Psi_{X-\\mu}(0) + \\Psi^{(1)}_{X - \\mu}(0)t + \\frac{\\Psi^{(2)}_{X - \\mu}(0)}{2}t^2 + o(t^2) = 1 + \\frac{\\sigma^2 t^2}{2} + o(t^2), \\text{ as } t \\to 0,\\] where \\(\\sigma^2 = \\mathbb{E}[(X - \\mu)^2]\\); recall that all moments are finite by Proposition 1. Similarly, for any \\(\\nu \u003e 0\\), \\begin{align*} \\exp{\\left\\{\\frac{\\nu^2 t^2}{2}\\right\\}} \u0026= \\left[1 + \\left\\{\\nu^2 s\\exp{\\left\\{\\frac{\\nu^2 s^2}{2}\\right\\}}\\right\\} t + \\frac{1}{2} \\left\\{\\nu^2 \\exp{\\left\\{\\frac{\\nu^2 s^2}{2}\\right\\}}\\left(1 + \\nu^2 s^2\\right)\\right\\} t^2\\right]_{s=0} + o(t^2) \\\\ \u0026= 1 + \\frac{\\nu^2 t^2}{2} + o(t^2), \\text{ as } t \\to 0, \\end{align*} as \\(e^x\\) is smooth. Combining these facts, we obtain for any \\(\\nu \u003e 0\\), \\[\\exp{\\left\\{\\frac{\\nu^2 t^2}{2}\\right\\}} - \\mathbb{E}(e^{t(X - \\mu)}) = \\frac{t^2}{2}(\\nu^2 - \\sigma^2)+ o(t^2), \\text{ as } t \\to 0.\\] To make sure this exceeds zero, at least for small \\(t\\), we will choose an arbitrary \\(\\nu \u003e \\sigma\\). Meanwhile, choose \\(\\delta \u003e 0\\) such that whenever \\(|t| \u003c \\delta\\), \\[\\left|\\frac{1}{t^2}\\left( \\exp{\\left\\{\\frac{\\nu^2 t^2}{2}\\right\\}} - \\mathbb{E}(e^{t(X - \\mu)})\\right) - \\frac{\\nu^2 - \\sigma^2}{2}\\right| \u003c \\epsilon,\\] where we choose \\(\\epsilon = \\frac{\\nu^2 - \\sigma^2}{4} \u003e 0\\). We then can see that \\[\\epsilon t^2 + \\mathbb{E}(e^{t(X - \\mu)}) \\leq \\exp{\\left\\{\\frac{\\nu^2 t^2}{2}\\right\\}} \\text{ for all } t \\in (-\\delta, \\delta),\\] so \\(X\\) is $(ν, \\frac{1}{\\delta})$-sub-exponential. \\(\\quad \\blacksquare\\) ","date":"2022-08-05","objectID":"/mgf-bounds/:3:0","tags":null,"title":"MGFs, power series, and pushing the expectation through","uri":"/mgf-bounds/"},{"categories":null,"content":"Control of the moments and Bernstein’s condition Viewing the formula for the MGF as a power series, we can obtain one more useful characterization of its existence in an open interval around zero. Proposition 3: The MGF of \\(X\\) is defined on \\(D \\supseteq (-c, c)\\) for some \\(c \u003e 0\\), if and only if \\[\\gamma \\coloneqq \\limsup_{n \\to \\infty} \\left(\\frac{\\left|\\mathbb{E}(X^n)\\right|}{n!}\\right)^{1/n} \u003c \\infty.\\] Proof: By our theorem, \\((-c, c) \\subseteq D\\) for some \\(c \u003e 0\\) if and only if \\(X^n \\in L_1\\) for all \\(n \\in \\mathbb{N}\\) and the radius of convergence \\(R\\) of the power series \\(\\sum_{n=0}^\\infty \\frac{t^n}{n!} \\mathbb{E}(X^n)\\) is positive. But \\(R = \\frac{1}{\\gamma}\\) by standard power series facts, so we conclude that \\(R \u003e 0\\) if and only if \\(\\gamma \u003c \\infty\\). \\(\\quad \\blacksquare\\) With this result in hand, we can more easily see that the Bernstein condition implies the sub-exponential property. Indeed, if \\(X\\) is $b$-Bernstein for some \\(b \u003e 0\\), then \\[\\limsup_{n \\to \\infty} \\left(\\frac{\\left|\\mathbb{E}[(X - \\mu)^n]\\right|}{n!}\\right)^{1/n} \\leq b \\limsup_{n \\to \\infty} \\left(\\frac{\\sigma^2}{2b^2}\\right)^{1/n} = b \u003c \\infty.\\] Thus, \\(\\left(-\\frac{1}{b}, \\frac{1}{b}\\right) \\subseteq D\\), again using that the domains of \\(\\Psi_X\\) and \\(\\Psi_{X - \\mu}\\) are both \\(D\\). By Proposition 2, we are done! ","date":"2022-08-05","objectID":"/mgf-bounds/:4:0","tags":null,"title":"MGFs, power series, and pushing the expectation through","uri":"/mgf-bounds/"},{"categories":null,"content":"BONUS: A direct analysis of the Bernstein condition In talking through this with some peers at CMU, I learned (thanks to Tiger Zeng) that we could also directly verify that the Bernstein condition implies \\(\\mathbb{E}(e^{|tX|}) \u003c \\infty\\) for \\(t \\in \\left(-\\frac{1}{b}, \\frac{1}{b}\\right)\\). At that point we could just use the DCT to justify the interchange of expectation and sum. Since the idea was pretty clever, I’ve decided to write it out here. Suppose \\(X\\) is $b$-Bernstein for \\(b \u003e 0\\). We wish to show that \\(\\mathbb{E}(e^{|t| \\cdot |X-\\mu|}) \u003c \\infty\\) for any \\(t \\in \\left(-\\frac{1}{b}, \\frac{1}{b}\\right)\\), and recall that by the Taylor series for \\(e^x\\) and the MCT, this expectation always equals \\(\\sum_{n=0}^{\\infty} \\frac{|t|^n}{n!}\\mathbb{E}(|X-\\mu|^n)\\). The difficulty comes from the fact that the Bernstein condition does not speak directly to \\(\\mathbb{E}(|X-\\mu|^n)\\), but rather to \\(\\left|\\mathbb{E}[(X-\\mu)^n]\\right|\\). For \\(n\\) even, these coincide, but for \\(n\\) odd, some care is needed. Explicitly separating the even and odd terms, we have \\[\\mathbb{E}(e^{|t| \\cdot |X-\\mu|}) = \\sum_{k=0}^\\infty \\frac{|t|^{2k}}{(2k)!}\\mathbb{E}(|X-\\mu|^{2k}) + \\sum_{k=0}^\\infty \\frac{|t|^{2k + 1}}{(2k + 1)!}\\mathbb{E}(|X-\\mu|^{2k + 1}).\\] We show these sums are both finite in turn. Starting with the even terms, since \\(X\\) is $b$-Bernstein, \\[\\sum_{k=0}^\\infty \\frac{t^{2k}}{(2k)!}\\mathbb{E}[(X-\\mu)^{2k}] = 1 + \\sum_{k=1}^\\infty \\frac{t^{2k}}{(2k)!}\\mathbb{E}[(X-\\mu)^{2k}] \\leq 1 + \\frac{\\sigma^2}{2} \\sum_{k=1}^{\\infty}t^{2k}b^{2k - 2} = 1 + \\frac{\\sigma^2 t^2 / 2}{1 - t^2b^2},\\] by summing the geometric series, noting that \\(t^2 b^2 = (|t|b)^2 \u003c 1\\) by assumption. For the odd terms, the trick is to forcefully introduce even moments using Cauchy-Schwarz: for any \\(k \\in \\mathbb{N}\\), \\[\\mathbb{E}(|X-\\mu|^{2k + 1}) = \\mathbb{E}(|X - \\mu|^{k + 1} |X - \\mu|^k) \\leq \\left\\{\\mathbb{E}[(X - \\mu)^{2k + 2}]\\mathbb{E}[(X - \\mu)^{2k}]\\right\\}^{1/2},\\] by Cauchy-Schwarz. Thus, as \\(X\\) is $b$-Bernstein, \\[\\mathbb{E}(|X-\\mu|^{2k + 1}) \\leq \\frac{\\sigma^2 b^{2k-1}}{2} \\sqrt{(2k)! (2k+2)!} \\leq (2k + 1)! \\sigma^2 b^{2k - 1},\\] since \\(\\sqrt{(2k)! (2k+2)!} = (2k + 1)! \\sqrt{\\frac{2k+2}{2k + 1}} \u003c 2 (2k+1)!\\) for all \\(k \\in \\mathbb{N}\\). Therefore, \\[\\sum_{k=0}^\\infty \\frac{|t|^{2k+1}}{(2k+1)!}\\mathbb{E}(|X-\\mu|^{2k+1}) \\leq \\mathbb{E}(|X - \\mu|) + \\sigma^2\\sum_{k=1}^{\\infty}|t|^{2k+1}b^{2k-1} = \\mathbb{E}(|X - \\mu|) + \\frac{b \\sigma^2 |t|^3}{1 - |t|b}.\\] Putting everything together, we have \\(\\mathbb{E}(e^{|t| \\cdot |X-\\mu|}) \u003c \\infty\\). Phew! ","date":"2022-08-05","objectID":"/mgf-bounds/:5:0","tags":null,"title":"MGFs, power series, and pushing the expectation through","uri":"/mgf-bounds/"},{"categories":null,"content":"short bio","date":"2022-07-19","objectID":"/about/","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"I am currently a postdoctoral researcher in the Statistics \u0026 Data Science department at Carnegie Mellon University. Before this, I completed a PhD in Biostatistics at Harvard University, and obtained a MSc in Biostatistics and BSc in Pharmacology from McGill University. Broadly, I am interested in causal inference and missing data, and have a penchant for statistical methods backed by semiparametric efficiency theory. Check out my research page to see what problems I have worked on and the areas I am interested in—or feel free to check out my CV, arXiv papers or Google Scholar profile. This site also serves as a platform for hosting teaching materials as well as short posts about statistics, math, or other topics. ","date":"2022-07-19","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]