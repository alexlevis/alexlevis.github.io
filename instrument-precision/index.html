<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>To adjust or not to adjust: instruments and precision variables - Alex Levis</title><meta name="Description" content="Welcome!"><meta property="og:url" content="https://alexlevis.github.io/instrument-precision/">
  <meta property="og:site_name" content="Alex Levis">
  <meta property="og:title" content="To adjust or not to adjust: instruments and precision variables">
  <meta property="og:description" content="Recently, I’ve been listening to (i.e., binging) the Casual Inference podcast hosted by Lucy D’Agostino McGowan and Ellie Murrary, and a question was raised at least a couple of times (Season 3 Episode 10 &amp; Season 4 Episode 9): does adjusting for an instrument (in, e.g., a propensity score model) cause bias? I believe the answer to this question is no, except in somewhat extreme scenarios. However, such a choice will impact the efficiency of a treatment effect estimator.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-04-29T00:00:00-05:00">
    <meta property="article:modified_time" content="2024-04-29T00:00:00-05:00">
    <meta property="og:image" content="https://alexlevis.github.io/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://alexlevis.github.io/logo.png">
  <meta name="twitter:title" content="To adjust or not to adjust: instruments and precision variables">
  <meta name="twitter:description" content="Recently, I’ve been listening to (i.e., binging) the Casual Inference podcast hosted by Lucy D’Agostino McGowan and Ellie Murrary, and a question was raised at least a couple of times (Season 3 Episode 10 &amp; Season 4 Episode 9): does adjusting for an instrument (in, e.g., a propensity score model) cause bias? I believe the answer to this question is no, except in somewhat extreme scenarios. However, such a choice will impact the efficiency of a treatment effect estimator.">
      <meta name="twitter:site" content="@alexwlevis">
<meta name="application-name" content="awlevis.com">
<meta name="apple-mobile-web-app-title" content="awlevis.com"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://alexlevis.github.io/instrument-precision/" /><link rel="prev" href="https://alexlevis.github.io/mgf-bounds/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "To adjust or not to adjust: instruments and precision variables",
        "inLanguage": "en",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/alexlevis.github.io\/instrument-precision\/"
        },"genre": "posts","wordcount":  2333 ,
        "url": "https:\/\/alexlevis.github.io\/instrument-precision\/","datePublished": "2024-04-29T00:00:00-05:00","dateModified": "2024-04-29T00:00:00-05:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Alex Levis"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Alex Levis">Alex Levis</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/about/"><i class="fas fa-user"></i> About </a><a class="menu-item" href="/research/"><i class="fas fa-microscope"></i> Research </a><a class="menu-item" href="/teaching/"><i class="fas fa-graduation-cap"></i> Teaching </a><a class="menu-item" href="/posts/"><i class="fas fa-pen-square"></i> Posts </a><a class="menu-item" href="/cv.pdf"><i class="fas fa-list"></i> CV </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Alex Levis">Alex Levis</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/about/" title=""><i class="fas fa-user"></i>About</a><a class="menu-item" href="/research/" title=""><i class="fas fa-microscope"></i>Research</a><a class="menu-item" href="/teaching/" title=""><i class="fas fa-graduation-cap"></i>Teaching</a><a class="menu-item" href="/posts/" title=""><i class="fas fa-pen-square"></i>Posts</a><a class="menu-item" href="/cv.pdf" title=""><i class="fas fa-list"></i>CV</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">To adjust or not to adjust: instruments and precision variables</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Alex Levis</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-04-29">2024-04-29</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;2333 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;11 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#graphs-instruments-and-precision-variables">Graphs, Instruments, and Precision Variables</a></li>
    <li><a href="#the-aipw-estimating-function-and-its-variance">The AIPW estimating function and its variance</a></li>
    <li><a href="#do-not-adjust-for-instruments"><em>Do not</em> adjust for instruments!</a></li>
    <li><a href="#do-adjust-for-precision-variables"><em>Do</em> adjust for precision variables!</a></li>
    <li><a href="#perfect-predicition-of-treatment">Perfect predicition of treatment</a></li>
    <li><a href="#practical-takeaways">Practical takeaways</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>Recently, I&rsquo;ve been listening to (i.e., binging) the <a href="https://casualinfer.libsyn.com" target="_blank" rel="noopener noreffer ">Casual Inference</a>
podcast hosted by Lucy D&rsquo;Agostino McGowan and Ellie Murrary, and a
question was raised at least a couple of times (Season 3 Episode 10 &amp;
Season 4 Episode 9): does adjusting for an instrument (in, e.g., a
propensity score model) cause bias? I believe the answer to this
question is no, except in somewhat extreme scenarios. However, such a
choice will impact the <em>efficiency</em> of a treatment effect
estimator. In this post, I want to review a pair of lemmas from an
excellent paper by <a href="https://www.jmlr.org/papers/volume21/19-1026/19-1026.pdf" target="_blank" rel="noopener noreffer ">Rotnitzky &amp; Smucler (2020; JMLR)</a> to formally
address this and related issues.</p>
<h2 id="graphs-instruments-and-precision-variables">Graphs, Instruments, and Precision Variables</h2>
<p>Say we are interested in the effect of an exposure, \(A\), on an
outcome, \(Y\). In order to use to data to estimate the causal effect of
\(A\) on \(Y\), we typically aim to adjust for a sufficient set of
<em>confounders</em>, $\boldsymbol{L}$&mdash;informally, \(\boldsymbol{L}\) are the
common causes of \(A\) on \(Y\). Graphically, we can represent these
relationships on a causal directed acyclic graph (DAG):</p>
<p><a id="figure--fig:confounding-dag"></a></p>
<figure><img src="/images/confounding-dag.png"
    alt="Figure 1: Canonical observational study DAG"><figcaption>
      <p><span class="figure-number">Figure 1: </span>Canonical observational study DAG</p>
    </figcaption>
</figure>

<p>The DAG in Figure 1 encodes the assumption that, within levels of
\(\boldsymbol{L}\), the treatment is as good as randomized. In
counterfactual language, we would say that</p>
<p>\begin{equation}
A \perp\!\!\!\!\perp Y(a) \mid \boldsymbol{L},
\end{equation}</p>
<p>where \(Y(a)\) is the potential outcome that would occur under exposure
level \(A = a\). Graphically, \(\boldsymbol{L}\) blocks all <em>backdoor</em>
paths (i.e., those starting with an edge <em>into</em> the treatment, \(A
\leftarrow\)) from \(A\) to \(Y\). [ <strong>Aside</strong>: the connection between the
DAG and potential outcomes is not obvious, and requires one to
associate a structural model with the DAG. With the <a href="https://csss.uw.edu/files/working-papers/2010/wp100.pdf" target="_blank" rel="noopener noreffer ">FFRCISTG</a> model,
one can read off counterfactual independence statements like this
using <a href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=89bd91b714f35759968555a87da06ce773a77f2f" target="_blank" rel="noopener noreffer ">single world intervention graphs</a>&mdash;this is a discussion for
another day! ]</p>
<p>In a given scientific setting, the situation may be more complicated
than that in the above picture. That is, we might imagine a more
refined DAG with a greater number of pre-treatment variables depicted
and certain arrows missing due to domain knowledge. Instruments and
precision variables represent two special cases of such additional
variables. An <em>instrument</em>, simply put, is a pure predictor of the
exposure. In this context, we will say \(Z\) is an instrument if it
causes \(A\) but only has an effect on \(Y\) through \(A\): in DAG form,</p>
<p><a id="figure--fig:instrument-dag"></a></p>
<figure><img src="/images/instrument-dag.png"
    alt="Figure 2: Observational study with an instrument"><figcaption>
      <p><span class="figure-number">Figure 2: </span>Observational study with an instrument</p>
    </figcaption>
</figure>

<p>Note that I use the two-sided arrow (\(\leftrightarrow\)) between
\(\boldsymbol{L}\) and \(Z\), as the direction of association (or indeed
the existence of an exogenous common cause) does not affect our
discussion. A <em>precision variable</em>, on the other hand, is a pure
predictor of the outcome. That is, \(W\) is a precision variable if it
directly affects \(Y\), but is only associated with \(A\) through
associations with \(\boldsymbol{L}\):</p>
<p><a id="figure--fig:precision-dag"></a></p>
<figure><img src="/images/precision-dag.png"
    alt="Figure 3: Observational study with a precision variable"><figcaption>
      <p><span class="figure-number">Figure 3: </span>Observational study with a precision variable</p>
    </figcaption>
</figure>

<p>In these alternative scenarios, we are left with multiple valid
adjustment sets. That is, in the DAG in Figure 2, we will have \((1)\)
in addition to \(A \perp\!\!\!\!\perp Y(a) \mid \boldsymbol{L},
Z\). Meanwhile, in the DAG in Figure 3, we have \((1)\) in addition to \(A
\perp\!\!\!\!\perp Y(a) \mid \boldsymbol{L}, W\). In each case, we
would like to know: should we adjust only for \(\boldsymbol{L}\), or
should we include the additional variable as well?</p>
<h2 id="the-aipw-estimating-function-and-its-variance">The AIPW estimating function and its variance</h2>
<p>In order to make the above question concrete, we need to specify at
the very least (i) a precise causal estimand of interest, (ii) one or
more identification formulas for this estimand, and (iii) an estimator
for these functionals. We will assume that we observe a random sample
\(O_1, \ldots, O_n \overset{\mathrm{iid}}{\sim} \mathbb{P}\), where a
typical observation is \(O = (\boldsymbol{L}, A, Y)\) or
\((\boldsymbol{L}, Z, A, Y)\) or \((\boldsymbol{L}, W, A, Y)\), for the
three scenarios above, respectively. The estimand of interest, for our
purposes, will be the population average treatment effect (ATE),
$\mathbb{E}(Y(1) - Y(0))$&mdash;we will assume for simplicity that the
exposure is binary, \(A \in \{0,1\}\).</p>
<p>To proceed with identification (and consequently to have a hope at
estimation), we need one crucial <em>positivity</em> assumption. <br />
<br /></p>
<p><strong>Assumption (\(*\))</strong>: For any \(\boldsymbol{B} \subseteq O \setminus \{A,
Y\}\), \(\mathbb{P}[A = 1 \mid \boldsymbol{B}] \in (0,1)\) with
probability 1.<br />
<br /></p>
<p>In all three DAG scenarios presented above, and under Assumption
(\(*\)), the ATE is identified by the <em>statistical</em> functional
\[\chi(\mathbb{P}) = \mathbb{E}_{\mathbb{P}}(\mu_1(\boldsymbol{L}) -
\mu_0(\boldsymbol{L})),\] where \(\mu_a(\boldsymbol{L}) =
\mathbb{E}_{\mathbb{P}}(Y \mid \boldsymbol{L}, A = a)\); henceforth we
will suppress dependence on \(\mathbb{P}\) and often omit inputs to
functions when there is no ambiguity. In order to represent
alternative adjustment sets, we will augment our notation: define
\[\chi_{\boldsymbol{B}} = \mathbb{E}(\mu_{1, \boldsymbol{B}} - \mu_{0,
\boldsymbol{B}}),\] where \(\boldsymbol{B} \subseteq O \setminus \{A,
Y\}\), and \(\mu_{a, \boldsymbol{B}}(\boldsymbol{B}) \equiv \mathbb{E}(Y
\mid \boldsymbol{B}, A = a)\). The ATE, \(\mathbb{E}(Y(1) - Y(0))\),
equals to \(\chi_{\boldsymbol{B}}\) where \(\boldsymbol{B} =
\boldsymbol{L}\) in the DAG of Figure 1, \(\boldsymbol{B} =
\boldsymbol{L}\) or \((\boldsymbol{L}, Z)\) in the DAG of Figure 2, and
\(\boldsymbol{B} = \boldsymbol{L}\) or \((\boldsymbol{L}, W)\) in the DAG
of Figure 3.</p>
<p>With identification out of the way, we are left with the statistical
task of estimating these quantities. I tend to prefer, when possible,
estimators that are asymptotically &ldquo;optimal&rdquo;. In particular,
estimators based on <a href="https://arxiv.org/pdf/2107.00681" target="_blank" rel="noopener noreffer "><em>influence functions</em></a> have really <a href="https://arxiv.org/pdf/2203.06469" target="_blank" rel="noopener noreffer ">nice theoretical
properties</a>. In a nonparametric model, a given functional has a unique
influence function. That said, the presence of an instrument or
precision variable means that the statistical model is no longer
completely nonparametric, and is actually a proper semiparametric
model. Concretely, the absence of arrows in a DAG implies conditional
independence restrictions on the observed data distribution
\(\mathbb{P}\): in the DAG of Figure 2, \(Z \perp\!\!\!\!\perp Y \mid
\boldsymbol{L}, A\), while in the DAG of Figure 3, \(A
\perp\!\!\!\!\perp W \mid \boldsymbol{L}\). As a consequence, there are
infinitely many influence functions one may work with, and the best
choice&mdash;the <em>efficient influence function</em> (EIF)&mdash;is that with the
lowest variance. The variance of a functional&rsquo;s EIF represents a local
asymptotic minimax lower bound for estimation of that functional, and
thus one often aims to construct an estimator that attains this
variance bound asymptotically.</p>
<p>Our goals in this note are a tad more modest, and we will instead
consider estimators based solely on the <em>nonparametric</em> influence
functions of \(\chi_{\boldsymbol{B}}\) for different choices of
\(\boldsymbol{B}\). Let \(\pi_{a, \boldsymbol{B}}(\boldsymbol{B}) =
\mathbb{P}[A = a \mid \boldsymbol{B}]\) be the exposure probabilities
on the basis of variables \(\boldsymbol{B}\), and define</p>
<p>\begin{equation}
\phi_{\boldsymbol{B}} = \mu_{1, \boldsymbol{B}} - \mu_{0,
\boldsymbol{B}} + \frac{2A - 1}{\pi_{A, \boldsymbol{B}}}(Y - \mu_{A,
\boldsymbol{B}}),
\end{equation}</p>
<p>which is the (uncentered) nonparametric influence function of the
functional \(\chi_{\boldsymbol{B}}\). The influence function in \((2)\)
takes a familiar augmented inverse probability-weighted (AIPW)
form. As mentioned, we wish to construct estimators based on
\(\phi_{\boldsymbol{B}}\). This is a general task that we may revisit in
another post, but let us quickly review one simple procedure, which
will yield the <em>cross-fit one-step estimator</em>. We will split our data
into \((D_1^n, D_2^n)\) where each \(D_j^n\) has size \(n/2\), fit models \((
\widehat{\mu}_{0,\boldsymbol{B}}, \widehat{\mu}_{1, \boldsymbol{B}},
\widehat{\pi}_{1,\boldsymbol{B}})\) on \(D_1^n\), then compute
\[\widehat{\chi}_{\boldsymbol{B}}^{(1)} = \frac{2}{n}\sum_{O_i \in
D_2^n} \widehat{\phi}_{\boldsymbol{B}}(O_i),\] where
\[\widehat{\phi}_{\boldsymbol{B}} = \widehat{\mu}_{1,
\boldsymbol{B}} - \widehat{\mu}_{0, \boldsymbol{B}} + \frac{2A -
1}{\widehat{\pi}_{A, \boldsymbol{B}}}(Y - \widehat{\mu}_{A,
\boldsymbol{B}}).\] We then swap the roles of \(D_1^n\) and \(D_2^n\),
repeat the above procedure obtain
\(\widehat{\chi}_{\boldsymbol{B}}^{(2)}\), then compute the final
estimator \(\widehat{\chi}_{\boldsymbol{B}} =
\frac{\widehat{\chi}_{\boldsymbol{B}}^{(1)} +
\widehat{\chi}_{\boldsymbol{B}}^{(2)}}{2}\). Under <em>relatively</em> weak
and general nonparametric conditions [ one condition being a stronger
positivity assumption, and another being the following: \(\lVert
\widehat{\mu}_{a,\boldsymbol{B}} - \mu_{a,\boldsymbol{B}} \rVert \cdot
\lVert \widehat{\pi}_{1,\boldsymbol{B}} - \pi_{1,\boldsymbol{B}}
\rVert = o_{\mathbb{P}}(n^{-1/2})\) for \(a \in \{0,1\}\), where \(\lVert
f \rVert^2 = \int f(o)^2 \, d\mathbb{P}(o)\) ], one is able to obtain
the following asymptotic normality result:
\[\sqrt{n}\left(\widehat{\chi}_{\boldsymbol{B}} -
\chi_{\boldsymbol{B}}\right) \overset{d}{\to} \mathcal{N}(0,
\mathrm{Var}(\phi_{\boldsymbol{B}})).\] Thus, in large samples, for
any choice of \(\boldsymbol{B}\) the estimator described above is
approximately unbiased with variance roughly equal to
\(\frac{\mathrm{Var}(\phi_{\boldsymbol{B}})}{n}\). This gives us a
simple criterion for selecting an adjustment set \(\boldsymbol{B}\):
choose \(\boldsymbol{B}\) to minimize
\(\mathrm{Var}(\phi_{\boldsymbol{B}})\)! The following simple result
provides an explicit form for this variance.</p>
<p><strong>Lemma 1</strong>: \(\mathrm{Var}(\phi_{\boldsymbol{B}}) = \mathrm{Var}(\mu_{1,
\boldsymbol{B}} - \mu_{0, \boldsymbol{B}}) +
\mathbb{E}\left(\frac{\sigma_{A, \boldsymbol{B}}^2}{\pi_{A,
\boldsymbol{B}}^2}\right)\), where \(\sigma_{a,
\boldsymbol{B}}^2(\boldsymbol{B}) \equiv \mathrm{Var}(Y \mid
\boldsymbol{B}, A = a)\).</p>
<p><em>Proof</em>: Observe the following:</p>
<ol>
<li>\(\mathbb{E}\left(\frac{2A - 1}{\pi_{A, \boldsymbol{B}}}
(Y - \mu_{A,\boldsymbol{B}}) \mid \boldsymbol{B}, A\right) = 0\).</li>
<li>\(\mathrm{Cov}\left(\mu_{1, \boldsymbol{B}} -
\mu_{0, \boldsymbol{B}}, \frac{2A - 1}{\pi_{A, \boldsymbol{B}}} (Y -
\mu_{A,\boldsymbol{B}})\right) = 0\).</li>
<li>\(\mathrm{Var}\left(\frac{2A - 1}{\pi_{A, \boldsymbol{B}}} (Y -
\mu_{A,\boldsymbol{B}})\right) =
\mathbb{E}\left(\mathrm{Var}\left(\frac{2A - 1}{\pi_{A,
\boldsymbol{B}}} (Y - \mu_{A,\boldsymbol{B}}) \mid \boldsymbol{B},
A\right)\right) = \mathbb{E}\left(
\frac{\sigma_{A, \boldsymbol{B}}^2}{\pi_{A,\boldsymbol{B}}^2}\right)\).</li>
</ol>
<p>Note that observations 2. and 3. follow from observation 1. The result
then follows immediately. \(\quad \blacksquare\) <br />
<br /></p>
<p>In the following subsections, we write instruments and precision
variables using bold symbols, \(\boldsymbol{Z}\) and \(\boldsymbol{W}\),
to reflect that these can be collections (i.e., vectors) of
covariates.</p>
<h2 id="do-not-adjust-for-instruments"><em>Do not</em> adjust for instruments!</h2>
<p><strong>Lemma 2</strong>: (Adapted from Lemma 5 in <a href="https://www.jmlr.org/papers/volume21/19-1026/19-1026.pdf" target="_blank" rel="noopener noreffer ">Rotnitzky &amp; Smucler (2020)</a>) <br /></p>
<p>Suppose \(\boldsymbol{Z} \perp\!\!\!\!\perp Y \mid \boldsymbol{L},
A\). Then \(\mathrm{Var}(\phi_{\boldsymbol{L}, \boldsymbol{Z}}) \geq
\mathrm{Var}(\phi_{\boldsymbol{L}})\).</p>
<p><em>Proof</em>: Note that under the stated conditional independence
assumption, \(\mu_{a,\boldsymbol{L}, \boldsymbol{Z}} \equiv \mu_{a,\boldsymbol{L}}\)
and \(\sigma_{a, \boldsymbol{L}, \boldsymbol{Z}}^2 \equiv \sigma_{a,
\boldsymbol{L}}^2\).  We therefore have
\[\mathbb{E}(\phi_{\boldsymbol{L}, \boldsymbol{Z}} \mid \boldsymbol{L}, A, Y) =
\mu_{1, \boldsymbol{L}} - \mu_{0, \boldsymbol{L}} + (2A - 1)(Y -
\mu_{A, \boldsymbol{L}})\mathbb{E}\left(\frac{1}{\pi_{A,
\boldsymbol{L}, \boldsymbol{Z}}} \mid \boldsymbol{L}, A \right),\] again using the
fact that \(\boldsymbol{Z} \perp\!\!\!\!\perp Y \mid \boldsymbol{L}, A\). Using Lemma
3 below, we then have \(\mathbb{E}(\phi_{\boldsymbol{L}, \boldsymbol{Z}} \mid
\boldsymbol{L}, A, Y) = \phi_{\boldsymbol{L}}\). Thus, by the law of
total variance, \[\mathrm{Var}\left(\phi_{\boldsymbol{L}, \boldsymbol{Z}}\right) =
\mathrm{Var}\left(\phi_{\boldsymbol{L}}\right) +
\mathbb{E}\left(\mathrm{Var}(\phi_{\boldsymbol{L}, \boldsymbol{Z}} \mid
\boldsymbol{L}, A, Y)\right) \geq
\mathrm{Var}\left(\phi_{\boldsymbol{L}}\right).\] Explicitly, if you
like, \[\mathbb{E}\left(\mathrm{Var}( \phi_{\boldsymbol{L}, \boldsymbol{Z}} \mid
\boldsymbol{L}, A, Y)\right) = \mathbb{E}\left(\sigma_{A,
\boldsymbol{L}}^2 \cdot \mathrm{Var}\left(\frac{1}{\pi_{A,
\boldsymbol{L}, \boldsymbol{Z}}} \mid \boldsymbol{L}, A\right)\right),\] which is
certainly non-negative. \(\quad \blacksquare\) <br />
<br /></p>
<p><strong>Lemma 3</strong>: \(\mathbb{E}\left(\frac{1}{\pi_{A, \boldsymbol{L}, \boldsymbol{Z}}} \mid
\boldsymbol{L}, A \right) = \frac{1}{\pi_{A, \boldsymbol{L}}}\).</p>
<p><em>Proof</em>: For each \(a \in \{0,1\}\), observe that</p>
<p>\begin{align*}
\mathbb{E}\left(\frac{1}{\pi_{a, \boldsymbol{L}, \boldsymbol{Z}}} \mid
\boldsymbol{L}, A = a\right)\pi_{a, \boldsymbol{L}}
&amp;= \mathbb{E}\left(\frac{I(A = a)}{\pi_{a, \boldsymbol{L}, \boldsymbol{Z}}} \mid
\boldsymbol{L}\right) \\
&amp;= \mathbb{E}\left(\frac{\mathbb{E}(I(A = a) \mid \boldsymbol{L}, \boldsymbol{Z}) }{\pi_{a, \boldsymbol{L}, \boldsymbol{Z}}} \mid
\boldsymbol{L}\right) \\
&amp;= 1,
\end{align*}</p>
<p>which proves the result. \(\quad \blacksquare\) <br />
<br /></p>
<h2 id="do-adjust-for-precision-variables"><em>Do</em> adjust for precision variables!</h2>
<p><strong>Lemma 4</strong>: (Adapted from Lemma 4 in <a href="https://www.jmlr.org/papers/volume21/19-1026/19-1026.pdf" target="_blank" rel="noopener noreffer ">Rotnitzky &amp; Smucler (2020)</a>) <br /></p>
<p>Suppose \(A \perp\!\!\!\!\perp \boldsymbol{W} \mid \boldsymbol{L}\). Then
\(\mathrm{Var}(\phi_{\boldsymbol{L}, \boldsymbol{W}}) \leq
\mathrm{Var}(\phi_{\boldsymbol{L}})\).</p>
<p><em>Proof</em>: Note that under the stated conditional independence
assumption, \(\pi_{A,\boldsymbol{L}, \boldsymbol{W}} \equiv
\pi_{A,\boldsymbol{L}}\). Thus,</p>
<p>\begin{align*}
\phi_{\boldsymbol{L}}
&amp;= \phi_{\boldsymbol{L}, \boldsymbol{W}} +
\left(\frac{A}{\pi_{1, \boldsymbol{L}}} - 1\right)
(\mu_{1, \boldsymbol{L}, \boldsymbol{W}} - \mu_{1, \boldsymbol{L}}) -
\left(\frac{1 - A}{1 - \pi_{1, \boldsymbol{L}}} - 1\right)
(\mu_{0, \boldsymbol{L}, \boldsymbol{W}} - \mu_{0, \boldsymbol{L}})
\end{align*}</p>
<p>Noting that, as \(A \perp\!\!\!\!\perp \boldsymbol{W} \mid \boldsymbol{L}\),
\[\mathbb{E}\left(\left(\frac{A}{\pi_{1, \boldsymbol{L}}} - 1\right)
(\mu_{1, \boldsymbol{L}, \boldsymbol{W}} - \mu_{1, \boldsymbol{L}}) -
\left(\frac{1 - A}{1 - \pi_{1, \boldsymbol{L}}} - 1\right) (\mu_{0,
\boldsymbol{L}, \boldsymbol{W}} - \mu_{0, \boldsymbol{L}}) \mid \boldsymbol{L},
\boldsymbol{W}\right) = 0,\] it then follows that
\[\mathrm{Cov}\left(\phi_{\boldsymbol{L}, \boldsymbol{W}}, \left(\frac{A}{\pi_{1,
\boldsymbol{L}}} - 1\right) (\mu_{1, \boldsymbol{L}, \boldsymbol{W}} - \mu_{1,
\boldsymbol{L}}) - \left(\frac{1 - A}{1 - \pi_{1, \boldsymbol{L}}} -
1\right) (\mu_{0, \boldsymbol{L}, \boldsymbol{W}} - \mu_{0, \boldsymbol{L}})\right)
= 0.\] Thus, we have</p>
<p>\begin{align*}
&amp; \mathrm{Var}(\phi_{\boldsymbol{L}}) \\
&amp;= \mathrm{Var}(\phi_{\boldsymbol{L}, \boldsymbol{W}}) +
\mathrm{Var}\left(\left(\frac{A}{\pi_{1,
\boldsymbol{L}}} - 1\right)(\mu_{1, \boldsymbol{L}, \boldsymbol{W}} - \mu_{1, \boldsymbol{L}}) -
\left(\frac{1 - A}{1 - \pi_{1, \boldsymbol{L}}} - 1\right)
(\mu_{0, \boldsymbol{L}, \boldsymbol{W}} - \mu_{0, \boldsymbol{L}})\right) \\
&amp; \geq \mathrm{Var}(\phi_{\boldsymbol{L}, \boldsymbol{W}}),
\end{align*}</p>
<p>as variance is non-negative. \(\quad \blacksquare\) <br />
<br /></p>
<h2 id="perfect-predicition-of-treatment">Perfect predicition of treatment</h2>
<p>Our discussion thus far has relied on positivity for any given
adjustment set \(\boldsymbol{B}\), i.e., Assumption (\(*\)). What if this
assumption holds for \(\boldsymbol{B} = \boldsymbol{L}\), but is
violated for \(\boldsymbol{B} = (\boldsymbol{L}, \boldsymbol{Z})\)? That
is, what if we include an instrument (or set of instruments) that
results in <em>perfect</em> prediction of treatment status for some subgroup:
\[\mathbb{P}\left[\mathbb{P}[A = 1 \mid \boldsymbol{L},
\boldsymbol{Z}] \in \{0,1\}\right] &gt; 0.\] Well, unfortunately
identification breaks down because one of \(\mu_{1, \boldsymbol{L},
\boldsymbol{Z}}\) or \(\mu_{0, \boldsymbol{L}, \boldsymbol{Z}}\) will not
be well-defined with some positive probability, therefore
\(\chi_{\boldsymbol{L}, \boldsymbol{Z}}\) will not be well-defined.</p>
<p>More realistically, in my opinion, inclusion of very predictive
instruments may result in practical near-positivity violations, i.e.,
\(\mathbb{P}[A = 1 \mid \boldsymbol{L}, \boldsymbol{Z}]\) may be very
close to 0 or 1. If this occurs, our outcome model estimates can
become unstable when there is little data for one treatment level, and
moreover the asymptotic variance of \(\widehat{\chi}_{\boldsymbol{L},
\boldsymbol{Z}}\), \(\mathrm{Var}(\phi_{\boldsymbol{L},
\boldsymbol{Z}})\), may explode&mdash;as a fun exercise, inspect the
variance formulas in Lemmas 1 and 2 to get a feel for where exactly
this manifests.</p>
<h2 id="practical-takeaways">Practical takeaways</h2>
<p>In practice, one may not be sure <em>a priori</em> whether a given variable
is an <em>exact</em> instrument or precision variable. What should one do in
such cases? For putative precision variables (e.g., those thought to
be strongly predictive of the outcome, but maybe only weakly
predictive of exposure), it is clear that these should definitely be
measured if possible and included in an adjustment set. For putative
instruments, perhaps one should err on the side of ensuring a valid
analysis and include covariates that may only be weakly associated
with the outcome (after controlling for \(\boldsymbol{L}\) and
\(A\)). When one is absolutely sure on the basis of domain knowledge, an
instrument can be excluded to gain some efficiency. Given the
discussion in the last section, I would emphasize that care should be
taken when including very strong predictors of treatment (unless they
are also very strong predictors of the outcome, whereby we would be
forced to include them as confounders).</p>
<p>More broadly, the discussion in this note assumes we are in the
luxurious setting where we measure a sufficient set of confounders of
the exposure-outcome relationship. Often, one questions whether this
is even the case, and it is typically possible to posit some important
unmeasured confounders. If one believes they have an instrument, and
that it is more likely to be unconfounded than the exposure itself,
then one may be able to exploit this structure for an alternative
route towards partial or full identification of the exposure
effect&mdash;if you&rsquo;re interested, you might enjoy our work on
<a href="https://arxiv.org/abs/2301.12106" target="_blank" rel="noopener noreffer ">partial identification</a> and our
<a href="https://arxiv.org/abs/2402.09332" target="_blank" rel="noopener noreffer ">review paper on identification</a> using instrumental variables!</p>
<p>For a deeper dive into the issues discussed in this note, the
consideration of arbitrarily complicated DAGs, the extension to
time-varying treatments, and <em>much</em> more, see the paper by <a href="https://www.jmlr.org/papers/volume21/19-1026/19-1026.pdf" target="_blank" rel="noopener noreffer ">Rotnitzky &amp;
Smucler</a>.</p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2024-04-29</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Linkedin" data-sharer="linkedin" data-url="https://alexlevis.github.io/instrument-precision/"><i class="fab fa-linkedin fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/mgf-bounds/" class="prev" rel="prev" title="MGFs, power series, and pushing the expectation through"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>MGFs, power series, and pushing the expectation through</a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.129.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2024</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Alex Levis</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":30,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
